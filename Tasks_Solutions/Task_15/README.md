# ğŸ· Wine Classification â€“ Streamlit App

This task is part of my **100 Days of Buildables** journey, where I build practical and interactive ML apps using **Python** and **Streamlit**.  
This app classifies wines into **3 classes** based on their chemical composition using ensemble learning techniques â€” **Decision Tree (baseline)**, **Random Forest**, and **XGBoost** â€” trained on the **UCI Wine dataset**.

---

## ğŸš€ Current Task â€“ Ensemble Wine Classification App

**ğŸŒ Live Demo**  
ğŸ‘‰ View on Streamlit Cloud: *([Wine Class Predictor](https://100daysofbuildables-task15.streamlit.app/))*

---

## ğŸ“‚ Repository Structure

```

WineEnsembleApp/
â”‚
â”œâ”€â”€ task15.py                      # Streamlit app with live model training and prediction
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ best_model_.pkl                # Saved the best trained model
â””â”€â”€ Task_15.ipynb                  # Jupyter Notebook for model comparison & analysis

````

---

## ğŸ› ï¸ Getting Started

Follow these steps to run the project locally:

### 1. **Clone the repository**
```sh
git clone https://github.com/AqsaNajeeb/100DaysOfBuildables/Tasks_Solutions/Task_15.git
cd Task_15
````

### 2. **Install dependencies**

Itâ€™s recommended to use a virtual environment:

```sh
pip install -r requirements.txt
```

### 3. **Run the app**

```sh
streamlit run task15.py
```

The app will open in your browser at: [http://localhost:8501](http://localhost:8501)

---

## âœ… Requirements

* Python 3.9+
* Streamlit
* NumPy
* Pandas
* Scikit-learn
* XGBoost
* Matplotlib
* Seaborn

All dependencies are listed in `requirements.txt`.

---

## ğŸŒŸ About This Task

This task demonstrates the **power of ensemble learning** in improving model performance.
It compares **Decision Tree**, **Random Forest**, and **XGBoost** classifiers on the **Wine dataset**, evaluating them using metrics like **Accuracy**, **Precision**, **Recall**, and **F1-score**.

**Key Highlights:**

* âœ… Ensemble model comparison (Decision Tree, Random Forest, XGBoost)
* âœ… Real-time predictions via Streamlit interface
* âœ… Auto-selection of best-performing model
* âœ… Scaled input preprocessing for consistent predictions
* âœ… Clean, professional UI with labeled feature inputs

---

## ğŸ“Š Model Evaluation Summary

| Model                    | Accuracy | Precision | Recall | F1-Score |
| ------------------------ | -------- | --------- | ------ | -------- |
| Decision Tree (Baseline) | 0.94     | 0.95      | 0.94   | 0.94     |
| Random Forest            | 1.00     | 1.00      | 1.00   | 1.00     |
| XGBoost                  | 1.00     | 1.00      | 1.00   | 1.00     |

Both ensemble models achieved perfect accuracy without overfitting, validated via stratified splits and controlled parameters.

---

## ğŸ“š My Reflection

**What worked?**

âœ… Ensemble models clearly outperformed the baseline Decision Tree.

âœ… Streamlit made real-time predictions intuitive and interactive.

âœ… The use of `StandardScaler` helped stabilize model training and improve generalization.

**What didnâ€™t work?**

âš ï¸ XGBoost can be slower on small datasets due to higher model complexity.

âš ï¸ Some input features (like Malic Acid and Proline) have large value ranges requiring careful normalization.

**Most Insightful Visualization?**

ğŸ“Š The feature importance plots showed that **Flavanoids**, **Color Intensity**, and **Proline** were the strongest predictors of wine class.

ğŸ”¥ Seeing how both Random Forest and XGBoost agreed on top features made model interpretation more reliable.

---

## ğŸ¤ Connect With Me

If youâ€™d like to collaborate, discuss ideas, or share feedback, feel free to reach out:

* GitHub: [Aqsa Najeeb](https://github.com/AqsaNajeeb)
* LinkedIn: [Aqsa Najeeb](https://www.linkedin.com/in/aqsa-najeeb/)

---

âœ¨ If you like this project, donâ€™t forget to **star â­ the repo!**
