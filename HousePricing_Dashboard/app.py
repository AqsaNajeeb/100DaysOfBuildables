# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZEfVtYrvRlg9rzLJeZayQSYoMMV_aacZ
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

st.title("ðŸ¡ House Pricing Dashboard")

# Upload dataset
uploaded_file = st.file_uploader("Upload your dataset (CSV)", type=["csv"])
if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    st.success("Dataset loaded successfully!")
    st.write("Shape of dataset:", df.shape)
    st.dataframe(df.head())

    # Step 2: Cleaning Missing Values
    st.subheader("Cleaning Missing Values")
    st.write("Missing values before cleaning:")
    st.write(df.isna().sum())

    # Fill numeric with median, categorical with mode
    num_cols = df.select_dtypes(include=[np.number]).columns
    cat_cols = df.select_dtypes(include=['object']).columns

    for col in num_cols:
        df[col].fillna(df[col].median(), inplace=True)

    for col in cat_cols:
        df[col].fillna(df[col].mode()[0], inplace=True)

    st.write("Missing values after cleaning:")
    st.write(df.isna().sum())

    # Step 3: Summary Statistics
    st.subheader("Summary of Statistics")
    summary = pd.DataFrame({
        "mean": df[num_cols].mean(),
        "median": df[num_cols].median(),
        "std": df[num_cols].std()
    })
    st.dataframe(summary)

    # Step 4: Exploratory Data Analysis
    st.subheader("Step 4: Exploratory Data Analysis")

    st.write("Correlation Heatmap (Numerical Features):")
    fig, ax = plt.subplots(figsize=(8,6))
    sns.heatmap(df[num_cols].corr(), annot=False, cmap="coolwarm", ax=ax)
    st.pyplot(fig)

    # Step 5. Show dataset preview
    st.dataframe(df.head())

    # Step 6. Add visualizations
    st.subheader("Visualizations")

    # Histogram
    col = st.selectbox("Choose a column for histogram", num_cols)
    fig, ax = plt.subplots()
    sns.histplot(df[col], bins=30, kde=True, ax=ax)
    st.pyplot(fig)

    # Scatterplot
    x = st.selectbox("X-axis", num_cols, index=0)
    y = st.selectbox("Y-axis", num_cols, index=1)
    fig2, ax2 = plt.subplots()
    ax2.scatter(df[x], df[y], alpha=0.6)
    ax2.set_xlabel(x)
    ax2.set_ylabel(y)
    st.pyplot(fig2)

    # Bar chart (for categorical)
    if len(cat_cols) > 0:
        cat = st.selectbox("Choose categorical column", cat_cols)
        fig3, ax3 = plt.subplots()
        df[cat].value_counts().plot(kind="bar", ax=ax3)
        st.pyplot(fig3)

    # Step 7. Add filters/sliders
    st.sidebar.header("Filters")
    filtered_df = df.copy()

    for col in num_cols[:5]:
      min_val = float(df[col].min())
      max_val = float(df[col].max())
      val_range = st.sidebar.slider(f"{col}", min_val, max_val, (min_val, max_val))
      filtered_df = filtered_df[(filtered_df[col] >= val_range[0]) & (filtered_df[col] <= val_range[1])]

    st.write("Filtered dataset preview:")
    st.dataframe(filtered_df.head())

    # Step 8. Modeling (Regression if numeric target)
    st.subheader("Regression Modeling")

    target = st.selectbox("Choose numeric target column", num_cols)
    features = [c for c in num_cols if c != target]

    X = filtered_df[features]
    y = filtered_df[target]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_reg = RandomForestRegressor(n_estimators=200, random_state=42)
    rf_reg.fit(X_train, y_train)
    y_pred = rf_reg.predict(X_test)

    st.write("MAE:", mean_absolute_error(y_test, y_pred))
    st.write("RMSE:", mean_squared_error(y_test, y_pred))
    st.write("RÂ²:", r2_score(y_test, y_pred))

    # Save feature names for prediction consistency
    feature_names = X_train.columns.tolist()

    # Step 9. Classification (if categorical target)
    st.subheader("Classification Modeling")

    if len(cat_cols) > 0:
      target_cat = st.selectbox("Choose categorical target column", cat_cols)
      features_cat = [c for c in df.columns if c != target_cat]
      X = pd.get_dummies(filtered_df[features_cat], drop_first=True)
      y = filtered_df[target_cat]

      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

      rf_clf = RandomForestClassifier(n_estimators=200, random_state=42)
      rf_clf.fit(X_train, y_train)
      y_pred = rf_clf.predict(X_test)

      st.write("Accuracy:", accuracy_score(y_test, y_pred))
      st.text("Classification Report:")
      st.text(classification_report(y_test, y_pred))
    else:
      st.info("No categorical columns available for classification.")


    # Step 10. Display predictions
    st.subheader("Make Predictions")

    input_data = {}
    for col in feature_names:
        val = st.number_input(
            f"Enter {col}",
            float(df[col].min()),
            float(df[col].max()),
            float(df[col].mean())
        )
        input_data[col] = val

    input_df = pd.DataFrame([input_data])

    if st.button("Predict"):
        input_df = input_df.reindex(columns=feature_names, fill_value=0)
        prediction = rf_reg.predict(input_df)
        st.success(f"Predicted {target}: {prediction[0]:,.2f}")

    joblib.dump(rf_reg, "best_model.pkl")

